---
title: "Versatile Exploration of Data: cv"
author: "Maani Beigy"
date: "February 18, 2019"
output: html_document
bibliography: DescObs.bib
csl: apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Coefficient of Variation   
Coefficient of variation *($CV$)* is a measure of relative dispersion representing 
the degree of variability relative to the mean [@Albatineh2014]. 
Since cv is unitless, it is useful for comparison of variables with different 
units [@Albatineh2014]. It is also a measure of homogeneity. The *population* coefficient of variation is:    
$$CV = \frac{\sigma}{\mu},$$
where $\sigma$ is the population standard deviation and $\mu$ is the population mean. Almost always, we analyze data from samples but want to generalize it 
as the population's parameter [@Albatineh2014]. Its sample's estimate is given 
as:    
$$cv = \frac{sd}{\bar{X}}$$   
where $sd$ is the sample standard deviation, the square root of the unbiased estimator of population variance, and $\bar{X}$ is the sample mean. The 
corrected *cv* to account for the sample size is:
$$
cv_{corr} = cv * \biggl(1 - \frac{1}{4(n-1)} 
+ \frac{1}{n}cv^2 
+ \frac{1}{2 (n-1)^2} \biggr)
$$
There are various methods for the calculation of **confidence intervals (CI)** 
for *cv*. All of them are fruitful and have particular use cases. Some of them 
are model-based hence their usage depends the assumptions regarding the distribution of data. For sake of versatility, we cover almost all of these methods in `DescObs` package. Here, we explain them along with some examples:

### Kelley Confidence Interval        

Let us assume that *cv* follows a noncentral *t* distribution, when the parent  population of the scores is *normally-distributed*, with noncentrality 
($\lambda$) parameter:   
$$
\lambda = \frac{\sqrt{n}}{cv}
$$
with *v* degrees of freedom, where $v = n - 1$. 
Let $1 - \alpha$ be the CI coverage with $\alpha_L + \alpha_U = \alpha$ in which $\alpha_L$ is the the proportion of times that *cv* will be 
less than the lower confidence bound and $\alpha_U$ the proportion of times that
*cv* will be greater than the upper confidence bound in the CI procedure [@Kelley2007]. The lower confidence tile for $\lambda$ is is the noncentrality parameter that results in $t_{(1-\alpha_L,v,\lambda_L)}=\hat{\lambda}$ and the
upper confdence tile for $\lambda$ is is the noncentrality parameter that 
results in $t_{(\alpha_U,v,\lambda_U)}=\hat{\lambda}$, where $t_{(1-\alpha_L,v,\lambda_L)}=\hat{\lambda}$ is the value of noncentral *t* distribution at the $1-\alpha_L$ **quantile** with noncentrality parameter $\lambda_L$ and $t_{(\alpha_U,v,\lambda_U)}=\hat{\lambda}$ is the value of noncentral *t* distribution at the $\alpha_U$ **quantile** with noncentrality parameter $\lambda_U$, respectively [@Kelley2007].   
Afterwards, we transform the tiles of the confidence interval for $\lambda$, by
dividing the tiles by $\sqrt{n}$ and therafter inversing them; the CI limits of 
$cv$ will be obtaned:  
$$
p\left[\biggl(\frac{\lambda_U}{\sqrt{n}}\biggr)^{-1}
\le CV \le \biggl(\frac{\lambda_L}{\sqrt{n}}\biggr)^{-1}\right] = 1-\alpha
$$
where $p$ stands for *probability*. Thanks to package `MBESS` [@Kelley2018] for 
the computation of confidence limits for the noncentrality parameter from a *t* distribution (`conf.limits.nct`), $cv$ will be obtained as: 
```{r echo=FALSE, warning=FALSE, message=FALSE, include = FALSE}
cv <- function(
    x,
    na.rm = FALSE,
    digits = NULL,
    method = NULL,
    correction = TRUE,
    alpha = 0.05,
    R = NULL,
    ...
) {
    # library(MBESS)
    # require(dplyr)
    # require(SciViews)
    # require(boot)
    if (!is.numeric(x)) {
        stop("argument is not numeric: returning NA")
        return(NA_real_)
    }
    if (!is.vector(x)) {
        stop("x is not a vector")
        return(NA_real_)
    }
    na.rm <- na.rm  # removes NAs if TRUE
    if (is.null(digits)) {
        digits = 4
    }
    digits <- digits  # digits required for rounding
    method <- tolower(method)
    method <- match.arg(
        arg = method,
        choices = c(
            "kelley", "mckay", "miller", "vangel", " mahmoudvand_hassani",
            "equal_tailed", "shortest_length", "normal_approximation",
            "norm","basic", "perc", "bca", "all"
        ),
        several.ok = TRUE
    )
    if ("kelley" %in% method) {
        if (!require(MBESS)) {
            warning(
        "package 'MBESS' required to calculate Kelley's confidence interval"
            )
        }
    }
    if (is.na(method)) {
        stop("invalid confidence interval method")
    }
    cv <- (
        sd(x, na.rm = na.rm)/mean(x, na.rm = na.rm)
    )
    cv_corr <- cv * (
        (1 - (1/(4 * (length(x) - 1))) +
             (1/length(x)) * cv^2) +
            (1/(2 * (length(x) - 1)^2))
    )
    if ("kelley" %in% method && correction == FALSE) {
        ci <- MBESS::conf.limits.nct(
            ncp = sqrt(length(x))/cv_corr,
            df = length(x) - 1,
            conf.level = (1 - alpha)
        )
        est <- cv
        lower.tile <- unname(sqrt(length(x))/ci$Upper.Limit)
        upper.tile <- unname(sqrt(length(x))/ci$Lower.Limit)
    } else if (method == "kelley" && correction == TRUE) {
        ci <- MBESS::conf.limits.nct(
            ncp = sqrt(length(x))/cv_corr,
            df = length(x) - 1,
            conf.level = (1 - alpha)
        )
        est <- cv_corr
        lower.tile <- unname(sqrt(length(x))/ci$Upper.Limit)
        upper.tile <- unname(sqrt(length(x))/ci$Lower.Limit)
    } else if (method == "mckay" && correction == FALSE) {
        if (cv > 0.33) {
            warning("Confidence interval may be very approximate")
        }
        v <- length(x) - 1
        t1 <- qchisq(1 - alpha/2,v)/v
        t2 <- qchisq(alpha/2,v)/v
        u1 <- v*t1
        u2 <- v*t2
        est <- cv
        lower.tile <- cv/sqrt((u1/(v + 1) - 1 )*cv*cv + u1/v)
        upper.tile <- cv/sqrt((u2/(v + 1) - 1)*cv*cv + u2/v)
    } else if (method == "mckay" && correction == TRUE) {
        if (cv_corr > 0.33) {
            warning("Confidence interval may be very approximate")
        }
        v <- length(x) - 1
        t1 <- qchisq(1 - alpha/2,v)/v
        t2 <- qchisq(alpha/2,v)/v
        u1 <- v*t1
        u2 <- v*t2
        est <- cv_corr
        lower.tile <- cv_corr/sqrt((u1/(v + 1) - 1 )*cv_corr*cv_corr + u1/v)
        upper.tile <- cv_corr/sqrt((u2/(v + 1) - 1)*cv_corr*cv_corr + u2/v)
    } else if (method == "miller" && correction == FALSE) {
        v <- length(x) - 1
        z_alpha_over2 <- qnorm(1 - (alpha/2))
        u <- sqrt(
            (cv^2/v) * (0.5 + cv^2)
        )
        zu <- z_alpha_over2 * u
        est <- cv
        lower.tile <- cv - zu
        upper.tile <- cv + zu
    } else if (method == "miller" && correction == TRUE) {
        v <- length(x) - 1
        z_alpha_over2 <- qnorm(1 - (alpha/2))
        u <- sqrt(
            (cv_corr^2/v) * (0.5 + cv_corr^2)
        )
        zu <- z_alpha_over2 * u
        est <- cv_corr
        lower.tile <- cv_corr - zu
        upper.tile <- cv_corr + zu
    }



    if (method == "kelley" && correction == FALSE) {
        return(
            list(
                method = "cv with Kelley 95% CI",
                statistics = data.frame(
                    est = round(est * 100, digits = digits),
                    lower = round(lower.tile * 100, digits = digits),
                    upper = round(upper.tile * 100, digits = digits),
                    row.names = c(" ")
                )
            )
        )
    } else if (method == "kelley" && correction == TRUE) {
        return(
            list(
                method = "Corrected cv with Kelley 95% CI",
                statistics = data.frame(
                    est = round(est * 100, digits = digits),
                    lower = round(lower.tile * 100, digits = digits),
                    upper = round(upper.tile * 100, digits = digits),
                    row.names = c(" ")
                )
            )
        )
    } else if (method == "mckay" && correction == FALSE) {
        return(
            list(
                method = "cv with McKay 95% CI",
                statistics = data.frame(
                    est = round(est * 100, digits = digits),
                    lower = round(lower.tile * 100, digits = digits),
                    upper = round(upper.tile * 100, digits = digits),
                    row.names = c(" ")
                )
            )
        )
    } else if (method == "mckay" && correction == TRUE) {
        return(
            list(
                method = "Corrected cv with McKay 95% CI",
                statistics = data.frame(
                    est = round(est * 100, digits = digits),
                    lower = round(lower.tile * 100, digits = digits),
                    upper = round(upper.tile * 100, digits = digits),
                    row.names = c(" ")
                )
            )
        )
    } else if (method == "miller" && correction == FALSE) {
        return(
            list(
                method = "cv with Miller 95% CI",
                statistics = data.frame(
                    est = round(est * 100, digits = digits),
                    lower = round(lower.tile * 100, digits = digits),
                    upper = round(upper.tile * 100, digits = digits),
                    row.names = c(" ")
                )
            )
        )
    } else if (method == "miller" && correction == TRUE) {
        return(
            list(
                method = "Corrected cv with Miller 95% CI",
                statistics = data.frame(
                    est = round(est * 100, digits = digits),
                    lower = round(lower.tile * 100, digits = digits),
                    upper = round(upper.tile * 100, digits = digits),
                    row.names = c(" ")
                )
            )
        )
    }
}
```

```{r eval = TRUE, warning=FALSE, message=FALSE}
x <- c(
    0.2, 0.5, 1.1, 1.4, 1.8, 2.3, 2.5, 2.7, 3.5, 4.4,
    4.6, 5.4, 5.4, 5.7, 5.8, 5.9, 6.0, 6.6, 7.1, 7.9
)
cv(
    x, 
    na.rm = TRUE, 
    digits = 3, 
    method = "kelley", 
    correction = TRUE, 
    alpha = 0.05
)
```
### McKay Confidence Interval
McKay [@McKay1932] introduced the following confidence interval for $cv$; considering $u_1 = \chi_{v,1-\alpha/2}^2$ and $u_1 = \chi_{v,\alpha/2}^2$ being
the $100(1-\alpha/2)\%$ and $100(\alpha/2)\%$ percentile of the $\chi^2$ distribution with $v = n-1$ degrees of freedom, respectively [@Albatineh2014]:   
$$
\biggl(cv\left[\biggl(\frac{u_1}{v}-1\biggr)(cv)^{2}+\frac{u_1}{v}\right]^{-1/2}
\le CV \le cv
\left[\biggl(\frac{u_2}{v}-1\biggr)(cv)^{2}+\frac{u_2}{v}\right]^{-1/2}\biggr)
$$
Let us calculate the 95\% CI for our variable $x$ according to McKay's method [@McKay1932]:
```{r eval = TRUE, warning=FALSE, message=FALSE}
x <- c(
    0.2, 0.5, 1.1, 1.4, 1.8, 2.3, 2.5, 2.7, 3.5, 4.4,
    4.6, 5.4, 5.4, 5.7, 5.8, 5.9, 6.0, 6.6, 7.1, 7.9
)
cv(
    x, 
    na.rm = TRUE, 
    digits = 3, 
    method = "mckay", 
    correction = TRUE, 
    alpha = 0.05
)
```
### Miller Confidence Interval    
Miller [@EdwardMiller1991] introduced the following confidence interval for 
$cv$; considering $Z_{\alpha/2}$ being the $(1-\alpha/2)$ percentile of the standard normal distribution [@Albatineh2014]:
$$
\biggl(cv - Z_{\alpha/2}\sqrt{
\biggl(\frac{cv^2}{v}\biggr)\biggl(\frac{1}{2}+cv^2\biggr)} \le
CV \le cv + Z_{\alpha/2}\sqrt{
\biggl(\frac{cv^2}{v}\biggr)\biggl(\frac{1}{2}+cv^2\biggr)}
\biggr)
$$
where $v = n-1$ is the degree of freedom.   
Let us calculate the 95\% CI for $x$ according to Miller's method [@EdwardMiller1991]:   
```{r eval = TRUE, warning=FALSE, message=FALSE}
x <- c(
    0.2, 0.5, 1.1, 1.4, 1.8, 2.3, 2.5, 2.7, 3.5, 4.4,
    4.6, 5.4, 5.4, 5.7, 5.8, 5.9, 6.0, 6.6, 7.1, 7.9
)
cv(
    x, 
    na.rm = TRUE, 
    digits = 3, 
    method = "miller", 
    correction = TRUE, 
    alpha = 0.05
)
```
# References   

